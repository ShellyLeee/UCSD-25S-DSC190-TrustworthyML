{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShellyLeee/UCSD-DSC-190-Trustworthy-Machine-Learning/blob/main/PA_Part_I.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjRM13EAuL3g"
      },
      "source": [
        "# ***PA - Part I: Basic Vision Models [Experiments with MNIST]*** (55pt)\n",
        "\n",
        "**Keywords**: Multiclass Image Classification, Neural Networks, PyTorch\n",
        "\n",
        "**MNIST**\n",
        "* The [MNIST](https://en.wikipedia.org/wiki/MNIST_database) database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems.\n",
        "* The MNIST database contains 70,000 labeled images. Each datapoint is a $28\\times 28$ pixels grayscale image.\n",
        "* However to speed up computations, we will use a much smaller dataset with size $8\\times 8$ images. These images are loaded from `sklearn.datasets`.\n",
        "\n",
        "**Agenda**:\n",
        "* The PA is split into three parts, the first part dealing with miniature models which we will build from scratch, the second part dealing with modern architectures and the bonus third part dealing with training vision models robust to adversarial attacks.\n",
        "* In this part, we will be performing multiclass classification on the simplified MNIST dataset from scratch.\n",
        "* We will be applying and analysing different loss functions, optimization techniques and learning methods.\n",
        "\n",
        "* We will be using PyTorch to do most of the heavylifting for modeling and training.\n",
        "\n",
        "**Note:**\n",
        "* Hardware acceleration (GPU) is recommended but not required for this part.\n",
        "* A note on working with GPU:\n",
        "  * Take care that whenever declaring new tensors, set `device=device` in parameters.\n",
        "  * You can also move a declared torch tensor/model to device using `.to(device)`.\n",
        "  * To move a torch model/tensor to cpu, use `.to('cpu')`\n",
        "  * Keep in mind that all the tensors/model involved in a computation have to be on the same device (CPU/GPU).\n",
        "* Run all the cells in order.\n",
        "* Only **add your code** to cells marked with \"TODO:\" or with \"...\"\n",
        "* You should not have to change variable names where provided, but you are free to if required for your implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUJgYttwjmpK"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSpZjZUd9iF4"
      },
      "source": [
        "### ***Setup: Imports and Utils***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNtGmOhl1I_m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "599ea067-73bb-444d-ce67-38fae4531c47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of images: 1797\n",
            "Number of features per image: 64\n"
          ]
        }
      ],
      "source": [
        "# imports\n",
        "import torch\n",
        "import numpy as np\n",
        "import math\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# loading the dataset directly from the scikit-learn library\n",
        "dataset = load_digits()\n",
        "X = dataset.data\n",
        "y = dataset.target\n",
        "print('Number of images:', X.shape[0])\n",
        "print('Number of features per image:', X.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDy8xSdXyJI3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "3840013b-8321-4e23-e274-bf8543dfb04c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# utility function to plot gallery of images\n",
        "def plot_gallery(images, titles, height, width, n_row=2, n_col=4):\n",
        "    plt.figure(figsize=(2* n_col, 3 * n_row))\n",
        "    plt.subplots_adjust(bottom=0, left=0.01, right=0.99, top=0.90, hspace=0.35)\n",
        "    for i in range(n_row * n_col):\n",
        "        plt.subplot(n_row, n_col, i + 1)\n",
        "        plt.imshow(images[i].reshape((height, width)), cmap=plt.cm.gray)\n",
        "        plt.title(titles[i], size=12)\n",
        "        plt.xticks(())\n",
        "        plt.yticks(())\n",
        "\n",
        "# visualize some of the images of the MNIST dataset\n",
        "plot_gallery(X, y, 8, 8, 2, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLRlr0wNOKI1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35c2ec50-a737-413d-a59a-a4ed52eeca35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of train dataset: (1437, 64)\n",
            "Shape of evaluation dataset: (360, 64)\n"
          ]
        }
      ],
      "source": [
        "# Let us split the dataset into training and test sets in a stratified manner.\n",
        "# Note that we are not creating evaluation datset as we will not be tuning hyper-parameters\n",
        "# The split ratio is 4:1\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "print('Shape of train dataset:', X_train.shape)\n",
        "print('Shape of evaluation dataset:', X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6Jam6RdFeIm"
      },
      "outputs": [],
      "source": [
        "# define some constants - useful for later\n",
        "num_classes = len(np.unique(y)) # number of target classes = 10 -- (0,1,2,3,4,5,6,7,8,9)\n",
        "num_features = X.shape[1]       # number of features = 64\n",
        "max_epochs = 100000             # max number of epochs for training\n",
        "lr = 1e-2                       # learning rate\n",
        "tolerance = 1e-6                # tolerance for early stopping during training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgMgGeeUUFb-"
      },
      "outputs": [],
      "source": [
        "# Hardware Acceleration: to set device if using GPU.\n",
        "# You can change runtime in colab by naviagting to (Runtime->Change runtime type), and selecting GPU in hardware accelarator.\n",
        "# NOTE that you can run this homework without GPU.\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBvXcyFhmLvU"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Va9kDfkC00K-"
      },
      "source": [
        "### *(a) Utils and pre-processing* (5pt)\n",
        "\n",
        "* Scale the image data between 0 and 1\n",
        "* One-hot encode the target data\n",
        "* Convert the data to tensors and move them to the required device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQGB7Y9cUO2V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9cf7385-78f1-4b22-dc64-e37cb8ba175a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.     0.     0.6875 ... 0.125  0.     0.    ]\n",
            " [0.     0.     0.125  ... 0.0625 0.     0.    ]\n",
            " [0.     0.125  0.9375 ... 0.     0.     0.    ]\n",
            " ...\n",
            " [0.     0.     0.0625 ... 0.1875 0.     0.    ]\n",
            " [0.     0.     0.25   ... 0.     0.     0.    ]\n",
            " [0.     0.     0.1875 ... 1.     1.     0.1875]]\n",
            "[[0.         0.         0.5        ... 0.25       0.         0.        ]\n",
            " [0.         0.42857143 1.         ... 0.5        0.1875     0.        ]\n",
            " [0.         0.         0.125      ... 0.0625     0.         0.        ]\n",
            " ...\n",
            " [0.         0.         0.625      ... 0.1875     0.         0.        ]\n",
            " [0.         0.         0.4375     ... 0.75       0.         0.        ]\n",
            " [0.         0.         0.         ... 0.125      0.         0.        ]]\n"
          ]
        }
      ],
      "source": [
        "# 1. Scale the features between 0 and 1\n",
        "# To scale, you can directly use the MinMaxScaler from sklearn.\n",
        "#######\n",
        "#TODO:\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler(feature_range = (0,1))\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.fit_transform(X_test)\n",
        "\n",
        "# output variable names -  X_train, X_test\n",
        "#######\n",
        "print(X_train)\n",
        "print(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZRF8HumUlen",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dba7f25-e939-4a28-bb6a-ca4e788d96b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of y_train_ohe: (1437, 10)\n",
            "Shape of y_test_ohe: (360, 10)\n"
          ]
        }
      ],
      "source": [
        "# 2. One-Hot encode the target labels\n",
        "# To one-hot encode, you can use the OneHotEncoder from sklearn\n",
        "#######\n",
        "#TODO:\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "ohe = OneHotEncoder()\n",
        "y_train_ohe = ohe.fit_transform(y_train.reshape(-1,1)) # Reshape: ohe need a 2d-array input\n",
        "y_test_ohe = ohe.fit_transform(y_test.reshape(-1,1))\n",
        "\n",
        "# output variable names -  y_train_ohe, y_test_ohe\n",
        "#######\n",
        "print('Shape of y_train_ohe:',y_train_ohe.shape)\n",
        "print('Shape of y_test_ohe:',y_test_ohe.shape)\n",
        "\n",
        "\n",
        "# move X and y to the defined device and convert them to torch.float32\n",
        "X_train_torch = ...\n",
        "X_test_torch = ...\n",
        "y_train_ohe_torch = ...\n",
        "y_test_ohe_torch = ...\n",
        "\n",
        "# output variable names -  X_train_torch, X_test_torch, y_train_ohe_torch, y_test_ohe_torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhpUZN-ES_e0"
      },
      "source": [
        "### *(b) Implement Multi-Class Logistic Regression from scratch* (15pt)\n",
        "\n",
        "In this problem, we will apply multiclass logistic regression from scratch  trained using gradient descent (GD) with Mean Squared Error (MSE) loss as the objective.\n",
        "\n",
        "We will be using a linear model $y^{(i)} = W \\mathbf{x}^{(i)}$, with the following notations:\n",
        "\n",
        "* Weight matrix:\n",
        "$ W_{p\\times n}= \\begin{bmatrix}\n",
        "\\leftarrow & \\mathbf{w}_1^\\top & \\rightarrow \\\\\n",
        "\\leftarrow & \\mathbf{w}_2^\\top & \\rightarrow\\\\\n",
        " & \\vdots &  \\\\\n",
        "\\leftarrow & \\mathbf{w}_p^\\top & \\rightarrow \\\\\n",
        "\\end{bmatrix}$, where $p$ is the number of target classes\n",
        "\n",
        "* data points: $\\mathbf{x}^{(i)}\\in \\mathbb{R}^n, y^{(i)}\\in \\mathbb{R}$, and $X = \\begin{bmatrix}\n",
        "\\uparrow &  \\uparrow & \\dotsm &  \\uparrow\\\\\n",
        "\\mathbf{x}^{(1)} & \\mathbf{x}^{(2)} & \\dotsm & \\mathbf{x}^{(m)} \\\\\n",
        "\\downarrow & \\downarrow  & \\dotsm & \\downarrow  \\\\\n",
        "\\end{bmatrix}, Y = \\begin{bmatrix}\n",
        "y^{(1)}\\\\\n",
        "y^{(2)} \\\\\n",
        "\\vdots \\\\\n",
        "y^{(m)} \\\\\n",
        "\\end{bmatrix}$, where $m$ is the number of datapoints."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJN3wY7CXe10"
      },
      "source": [
        "**Note:** Here we need to define the model prediction. The input matrix is $X_{n\\times m}$ where $m$ is the number of examples, and $n$ is the number of features. The linear predictions can be given by: $Y = WX + b$ where $W$ is a $p\\times n$ weight matrix and $b$ is a $p$ size bias vector. $p$ is the number of target classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AztFFkzUuHW1"
      },
      "source": [
        "#### #1. Define a function `linear_model`\n",
        "\n",
        "* This function takes as input a weight matrix (`W`), bias vector (`b`), and input data matrix of size $m\\times n$ (`XT`).\n",
        "* This function should return the predictions $\\hat{y}$, which is an $m\\times p$ matrix. For each datapoint row which is a $1 \\times p$ vector, the prediction can be seen as the scores that the model gives each target class for that datapoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75DOhP-EVsRs"
      },
      "outputs": [],
      "source": [
        "#######\n",
        "def linear_model(W, b, XT):\n",
        "    #TODO:\n",
        "    ...\n",
        "    return y_hat\n",
        "\n",
        "#######"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejbSxyl2du2L"
      },
      "source": [
        "**Note:** The loss function that we would be using is the Mean Square Error (L2) Loss:\\\n",
        "$\\displaystyle MSE = \\frac{1}{m}\\sum_{i=1}^m(\\hat{y}^{(i)}-y^{(i)})^2$, where $m$ is the number of examples, $\\hat{y}^{(i)}$ is the\n",
        "predicted value of $x^{(i)}$ and $y^{(i)}$ is the ground truth of $x^{(i)}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8snSDdp4u49o"
      },
      "source": [
        "#### #2. Define a function `mse_loss`\n",
        "\n",
        "* This function takes as input prediction (`y_pred`) and ground-truth label (`y`), and returns the MSE loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGjd_pYPduV0"
      },
      "outputs": [],
      "source": [
        "#######\n",
        "def mse_loss(y_pred, y):\n",
        "    #TODO:\n",
        "    ...\n",
        "    return loss\n",
        "\n",
        "#######"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d27EoDd1vIA7"
      },
      "source": [
        "\n",
        "#### #3. Define a function: `initializeWeightsAndBiases`\n",
        "\n",
        "In this part, we will do some setup required for training (such as initializing weights and biases) and move everything to torch tensors.\n",
        "\n",
        "\n",
        "* This function returns tuple `(W, b)`, where `W` is a randomly generated torch tensor of size `num_classes x num_features`, and `b` is a randomly generated torch vector of size `num_classes`.\n",
        "* For both the tensors, set `requires_grad=True` in parameters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yF-F0XbQjhYF"
      },
      "outputs": [],
      "source": [
        "#######\n",
        "def initializeWeightsAndBiases(num_classes, num_features):\n",
        "    #TODO:\n",
        "    W = ...\n",
        "    b = ...\n",
        "    return W, b\n",
        "#######"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpiEdcLYg6Vb"
      },
      "source": [
        "#### #4 Training code\n",
        "\n",
        "* Given below is a function: `train_linear_regression_model` that takes as inputs as max number of epochs (`max_epochs`), weights (`W`), biases (`b`), training data (`X_train, y_train`), learning rate (`lr`), tolerance for stopping (`tolerance`).\n",
        "* This function returns a tuple `(W,b,losses)` where `W,b` are the trained weights and biases respectively, and `losses` is a list of tuples of loss logged every $100^{th}$  epoch.\n",
        "* You can go through [this](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html#pytorch-tensors-and-autograd) article for reference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZrxj7NEJhiw"
      },
      "outputs": [],
      "source": [
        "# Define a function train_linear_regression_model\n",
        "def train_linear_regression_model(max_epochs, W, b, X_train, y_train, lr, tolerance):\n",
        "  #TODO:\n",
        "  losses = []\n",
        "  prev_loss = float('inf')\n",
        "\n",
        "  for epoch in tqdm(range(max_epochs)):\n",
        "\n",
        "      #######\n",
        "      # 7. do prediction\n",
        "      y_pred = ...\n",
        "\n",
        "      # 8. get the loss\n",
        "      loss = ...\n",
        "\n",
        "      # 9. backpropagate loss\n",
        "      ...\n",
        "\n",
        "      # 10. update the weights and biasees\n",
        "      ...\n",
        "\n",
        "      # 11. set the gradients to zero\n",
        "      ...\n",
        "\n",
        "      #######\n",
        "\n",
        "      # log the loss every 100th epoch and print every 5000th epoch:\n",
        "      if epoch%100==0:\n",
        "        losses.append((epoch, loss.item()))\n",
        "        if epoch%5000==0:\n",
        "          print('Epoch: {}, Loss: {}'.format(epoch, loss.item()))\n",
        "\n",
        "      # break if decrease in loss is less than threshold\n",
        "      ...\n",
        "\n",
        "  # return updated weights, biases, and logged losses\n",
        "  return W, b, losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFJ-tBGgeOdw"
      },
      "source": [
        "#### #5. Initialize parameters and train your own model\n",
        "\n",
        "* Initialize weights and biases using the `initializeWeightsAndBiases` function that you defined earlier\n",
        "* Train your model using function `train_linear_regression_model` defined above.\n",
        "* Use full batch (set `batch_size=len(X_train)` for training (Gradient Descent).\n",
        "* Also plot the graph of loss vs number of epochs (Recall that values for learning rate (`lr`) and tolerance (`tolerance`) are already defined above)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHfBioX0K294"
      },
      "outputs": [],
      "source": [
        "#######\n",
        "#TODO:\n",
        "W, b = ...\n",
        "W, b, losses = ...\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot([x[0] for x in losses],[x[1] for x in losses])\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.title('loss vs epochs')\n",
        "#######"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIDhQXpTYX9T"
      },
      "outputs": [],
      "source": [
        "# print accuracies of model\n",
        "predictions_train = ...\n",
        "predictions_test = ...\n",
        "y_train_pred = ...\n",
        "\n",
        "y_test_pred = ...\n",
        "print(\"Train accuracy:\",accuracy_score(y_train_pred, np.asarray(y_train, dtype=np.float32)))\n",
        "print(\"Test accuracy:\",accuracy_score(y_test_pred, np.asarray(y_test, dtype=np.float32)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iz8cNUXR5DOu"
      },
      "source": [
        "### *(c) Use PyTorch for training* (10pt)\n",
        "\n",
        "* In the previous part, we defined the model, loss, and even the gradient update step. We also had to manually set the gradients to zero.\n",
        "* In this part, we will re-implement the linear model and see how we can directly use Pytorch to do all this for us in a few simple steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lwm9t0K0lbvl"
      },
      "outputs": [],
      "source": [
        "# common utility function to print accuracies\n",
        "def print_accuracies_torch(model, X_train_torch, X_test_torch, y_train, y_test):\n",
        "  predictions_train = ...\n",
        "  predictions_test = ...\n",
        "  y_train_pred = ...\n",
        "  y_test_pred = ...\n",
        "  print(\"Train accuracy:\",accuracy_score(y_train_pred, np.asarray(y_train, dtype=np.float32)))\n",
        "  print(\"Test accuracy:\",accuracy_score(y_test_pred, np.asarray(y_test, dtype=np.float32)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSjn0TlT0_X5"
      },
      "source": [
        "#### #1. Define the linear model using PyTorch\n",
        "\n",
        "* Use the inbuilt PyTorch `torch.nn.Module` to define a model class.\n",
        "* Use the `torch.nn.Linear` to define the linear layer of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAVqYskI4ohI"
      },
      "outputs": [],
      "source": [
        "#######\n",
        "# Define a model class using torch.nn\n",
        "class Linear_Model(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Linear_Model, self).__init__()\n",
        "    # Initalize various layers of model as below\n",
        "    # 1. initialze one linear layer: num_features -> num_targets\n",
        "    ...\n",
        "\n",
        "  def forward(self, X):\n",
        "    # 2. define the feedforward algorithm of the model and return the final output\n",
        "    ...\n",
        "    return\n",
        "\n",
        "#######"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zyx8lVTFq8q"
      },
      "source": [
        "#### #2. A general function for training a PyTorch model.\n",
        "\n",
        "Define a general training function: `train_torch_model`.\n",
        "* This function takes as input an initialized torch model (`model`), batch size (`batch_size`), initialized loss (`criterion`), max number of epochs (`max_epochs`), training data (`X_train, y_train`), learning rate (`lr`), tolerance for stopping (`tolerance`).\n",
        "* This function will return a tuple `(model, losses)`, where `model` is the trained model, and `losses` is a list of tuples of loss logged every $100^{th}$ epoch.\n",
        "\n",
        "Note: You can reuse a lot of components from the `train_linear_regression_model` function from the previous section\n",
        "\n",
        "You can go through [this](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html#pytorch-optim) article for reference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4T04zQPE53rS"
      },
      "outputs": [],
      "source": [
        "# Define a function train_torch_model\n",
        "def train_torch_model(model, batch_size, criterion, max_epochs, X_train, y_train, lr, tolerance):\n",
        "  losses = []\n",
        "  prev_loss = float('inf')\n",
        "  number_of_batches = math.ceil(len(X_train)/batch_size)\n",
        "\n",
        "  #######\n",
        "  # 3. move model to device\n",
        "  ...\n",
        "\n",
        "  # 4. define optimizer (use torch.optim.SGD (Stochastic Gradient Descent))\n",
        "  # Set learning rate to lr and also set model parameters\n",
        "  optimizer =  ...\n",
        "\n",
        "  for epoch in tqdm(range(max_epochs)):\n",
        "    for i in range(number_of_batches):\n",
        "      X_train_batch = ...\n",
        "      y_train_batch = ...\n",
        "\n",
        "      # 5. reset gradients\n",
        "      ...\n",
        "\n",
        "      # 6. prediction\n",
        "      prediction = ...\n",
        "\n",
        "      # 7. calculate loss\n",
        "      loss = ...\n",
        "\n",
        "      # 8. backpropagate loss\n",
        "      ...\n",
        "\n",
        "      # 9. perform a single gradient update step\n",
        "      ...\n",
        "\n",
        "  #######\n",
        "\n",
        "    # log loss every 100th epoch and print every 5000th epoch:\n",
        "    if epoch%100==0:\n",
        "      losses.append((epoch, loss.item()))\n",
        "      if epoch%5000==0:\n",
        "        print('Epoch: {}, Loss: {}'.format(epoch, loss.item()))\n",
        "\n",
        "    # break if decrease in loss is less than threshold\n",
        "    ...\n",
        "\n",
        "  # return updated model and logged losses\n",
        "  return model, losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMXOadCpkufn"
      },
      "source": [
        "### *(d) Working with different model types* (15pt)\n",
        "* Now, we retrain the above model with `batch_size=64`\n",
        "* Use Stochastic/Mini-batch Gradient Descent and keep everything else the same.\n",
        "* Like before, we plot the graph between loss and number of epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7RzDMajj8sc"
      },
      "source": [
        "#### #1. MSE Loss and Gradient Descent (GD)\n",
        "* Use `nn.MSELoss`.\n",
        "* Use full batch for training (Gradient Descent).\n",
        "* Also plot the graph of loss vs number of epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDm0__pE6edF"
      },
      "outputs": [],
      "source": [
        "#######\n",
        "model = ...\n",
        "criterion = ...\n",
        "batch_size = ...\n",
        "model, losses = ...\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot([x[0] for x in losses],[x[1] for x in losses])\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.title('loss vs epochs')\n",
        "#######"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIMHwStJ8zzl"
      },
      "outputs": [],
      "source": [
        "# print accuracies of model\n",
        "print_accuracies_torch(model, X_train_torch, X_test_torch, y_train, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJ9wHXQx7uDg"
      },
      "source": [
        "#### #2. MSE Loss and Stochastic Gradient Descent (SGD)\n",
        "* Use `nn.MSELoss`.\n",
        "* Use `batch_size=64` for training\n",
        "* Also plot the graph of loss vs number of epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsj_apLOkt17"
      },
      "outputs": [],
      "source": [
        "#######\n",
        "model = ...\n",
        "criterion = ...\n",
        "batch_size = ...\n",
        "model, losses = ...\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot([x[0] for x in losses],[x[1] for x in losses])\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.title('loss vs epochs')\n",
        "#######"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJHvwtH6k0F6"
      },
      "outputs": [],
      "source": [
        "# print accuracies of model\n",
        "print_accuracies_torch(model, X_train_torch, X_test_torch, y_train, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYiv8O3emFgn"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMXvLv81HPCM"
      },
      "source": [
        "#### **Cross-Entropy (CE) Loss with Linear Model**\n",
        "* Instead of using MSE Loss, we will use a much more natural loss function for the multi-class logistic regression task which is the Cross Entropy Loss.\n",
        "* CE loss converts the model scores of each class into a probability.\n",
        "* This model penalizes both choosing the wrong class as well as uncertainty (choosing the right class with low probability).\n",
        "* And we will use the same linear model defined in **(c)**.\n",
        "\n",
        "\n",
        "**Note:**\n",
        "The [Cross Entropy Loss](https://ebookreading.net/view/book/EB9781789130331_73.html) for multiclass classification is the mean of the negative log likelihood of the output logits after softmax:\\\n",
        "$L = \\underbrace{\\frac{1}{m} \\sum_{i=1}^m \\underbrace{-y^{(i)} \\underbrace{log \\underbrace{\\frac{e^{\\hat{y}^{(i)}}}{\\sum_{j=1}^p e^{\\hat{y}^{(j)}}}}_{\\text{Softmax}}}_{\\text{LogSoftmax}}}_{\\text{Negative Log Likelihood (NLL)}}}_{\\text{Cross Entropy (CE) Loss }}$,\n",
        "\n",
        "\n",
        "where $y^{(i)}$ is the ground truth, and $\\hat{y}^{(k)}$ (also called as *logits*) represent the outputs of the last linear layer of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQw1h-A3lCPp"
      },
      "source": [
        "#### #3. CE Loss and GD\n",
        "* Instead of `nn.MSELoss`, train the linear model with `nn.CrossEntropyLoss`.\n",
        "* Use **full-batch**.\n",
        "* Also plot the graph between loss and number of epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXmF1XD_9Irh"
      },
      "outputs": [],
      "source": [
        "#######\n",
        "model = ...\n",
        "criterion = ...\n",
        "batch_size = ...\n",
        "model, losses = ...\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot([x[0] for x in losses],[x[1] for x in losses])\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.title('loss vs epochs')\n",
        "#######"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOvC4HdUINQt"
      },
      "outputs": [],
      "source": [
        "# print accuracies of model\n",
        "print_accuracies_torch(model, X_train_torch, X_test_torch, y_train, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGQ_G6tHl8XP"
      },
      "source": [
        "#### #4. CE Loss and SGD\n",
        "* Use a different batch size, `batch_size=64` with the new loss and repeat the previous part #1.\n",
        "* Also plot the graph of loss vs epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fNrbHH6IOiz"
      },
      "outputs": [],
      "source": [
        "#######\n",
        "model = ...\n",
        "criterion = ...\n",
        "batch_size = ...\n",
        "model, losses = ...\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot([x[0] for x in losses],[x[1] for x in losses])\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.title('loss vs epochs')\n",
        "#######"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0o04L-LNmCkp"
      },
      "outputs": [],
      "source": [
        "# print accuracies of model\n",
        "print_accuracies_torch(model, X_train_torch, X_test_torch, y_train, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HgoKWSV73xk"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flKPhgoccRcL"
      },
      "source": [
        "#### **Training a neural network model in PyTorch**\n",
        "\n",
        "* We will train a neural network in pytorch with two hidden layers of sizes 32 and 16 neurons.\n",
        "* We will use non-linear ReLU activations to effectively make it a non-linear model.\n",
        "* We will use this neural network model for multi-class classification with Cross Entropy Loss.\n",
        "\n",
        "**Note:** The neural network model output can be represented mathematically as below:\n",
        "$\\hat{y}^{(i)}_{10\\times1} = W^{(3)}_{10\\times 16}\\sigma(W^{(2)}_{16\\times 32}\\sigma(W^{(1)}_{32\\times 64}\\mathbf{x}^{(i)}_{64\\times1}+\\mathbf{b}^{(1)}_{32\\times1})+\\mathbf{b}^{(2)}_{16\\times1})+\\mathbf{b}^{(3)}_{10\\times1}$,\n",
        "where $\\sigma$ represents ReLU activation, $W^{(i)}$ is the weight of the $i^{th}$ linear layer, and $\\mathbf{b}^{(i)}$ is the layer's bias. We use the subscript to denote the dimension for clarity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXYnz6-eI8MG"
      },
      "source": [
        "#### #5. Define the 2 hidden-layer Neural Network (NN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3PSTz2jcQ0j"
      },
      "outputs": [],
      "source": [
        "#######\n",
        "# Define a neural network model class using torch.nn\n",
        "class NN_Model(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(NN_Model, self).__init__()\n",
        "    # Initalize various layers of model as instructed below\n",
        "    # 1. initialize three linear layers: num_features -> 32, 32 -> 16, 16 -> num_targets\n",
        "    ...\n",
        "\n",
        "    # 2. initialize RELU\n",
        "    ...\n",
        "\n",
        "  def forward(self, X):\n",
        "    # 3. define the feedforward algorithm of the model and return the final output\n",
        "    # Apply non-linear ReLU activation between subsequent layers\n",
        "    ...\n",
        "    return\n",
        "\n",
        "#######"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mERRi4Eamze0"
      },
      "source": [
        "#### #6. NN with CE Loss and GD\n",
        "* Use Cross Entropy Loss.\n",
        "* Use full-batch and plot the graph of loss vs number of epochs.\n",
        "* Note that you can re-use the training function `train_torch_model` (from part (b))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pliWaICdmzEB"
      },
      "outputs": [],
      "source": [
        "#######\n",
        "model = ...\n",
        "criterion = ...\n",
        "batch_size = ...\n",
        "model, losses = ...\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot([x[0] for x in losses],[x[1] for x in losses])\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.title('loss vs epochs')\n",
        "#######"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EoRJpn8QnZrX"
      },
      "outputs": [],
      "source": [
        "# print accuracies of model\n",
        "print_accuracies_torch(model, X_train_torch, X_test_torch, y_train, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TP-JX4IznLQL"
      },
      "source": [
        "#### #7. NN with CE Loss and SGD\n",
        "* Re-train the above model with `batch_size=64`.\n",
        "* Also plot the graph of loss vs epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sx8e5dvJnK1n"
      },
      "outputs": [],
      "source": [
        "#######\n",
        "model = ...\n",
        "criterion = ...\n",
        "batch_size = ...\n",
        "model, losses = ...\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot([x[0] for x in losses],[x[1] for x in losses])\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.title('loss vs epochs')\n",
        "#######"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mr7M3xyknaLp"
      },
      "outputs": [],
      "source": [
        "# print accuracies of model\n",
        "print_accuracies_torch(model, X_train_torch, X_test_torch, y_train, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szUeNm2kotqY"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJ1HXvh_ovQ_"
      },
      "source": [
        "### *(e) Analyze the results* (10pt)\n",
        "\n",
        "In the above few examples, we performed several experiments with different batch size and loss functions. Now it's time to analyze our observations from the results.\n",
        "\n",
        "Recall that we trained the following models in this tutorial:\n",
        "\n",
        "1.   Linear Model - Scratch + MSE Loss + Full Batch (GD)\n",
        "2.   Linear Model - PyTorch + MSE Loss + Full Batch\n",
        "3.   Linear Model - PyTorch + MSE Loss + Mini Batch (SGD)\n",
        "4.   Linear Model - PyTorch + CE Loss + Full Batch\n",
        "5.   Linear Model - PyTorch + CE Loss + Mini Batch\n",
        "6.   NN Model - PyTorch + CE Loss + Full Batch\n",
        "7.   NN Model - PyTorch + CE Loss + Mini Batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a23mYhUF0aEx"
      },
      "outputs": [],
      "source": [
        "# plotting graph of accuracy vs model\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "xlabels = ['linear-scratch', 'linear-torch-full', 'linear-torch-batch', 'linear-celoss-full', 'linear-celoss-batch', 'nn-full', 'nn-batch']\n",
        "train_accuracies = [..., ..., ..., ..., ..., ..., ...]\n",
        "test_accuracies = [..., ..., ..., ..., ..., ..., ...]\n",
        "\n",
        "tuples = [(xlabels[i],train_accuracies[i], 'train' ) for i in range(len(xlabels))]+[(xlabels[i],test_accuracies[i], 'test' ) for i in range(len(xlabels))]\n",
        "df = pd.DataFrame(tuples, columns = ['model', 'accuracy', 'type'])\n",
        "px.line(df, x='model', y='accuracy', color = 'type', markers=True, title = 'accuracy vs. model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6e4sulhvLp5"
      },
      "source": [
        "#### #1.Analysis\n",
        "**Effect of using full vs. batch gradient descent:**\n",
        "\n",
        "\n",
        "**Effect of using different loss strategy:**\n",
        "\n",
        "\n",
        "**Effect of using linear vs. non-linear models:**\n",
        "\n",
        "\n",
        "**Training time per epoch in different cases:**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "VSpZjZUd9iF4",
        "AztFFkzUuHW1",
        "8snSDdp4u49o",
        "d27EoDd1vIA7",
        "eMXOadCpkufn",
        "flKPhgoccRcL",
        "KXt-55xIAU5A",
        "_6QOUxb7jFtk"
      ],
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}